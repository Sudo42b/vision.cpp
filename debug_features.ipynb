{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c18ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b127ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from scripts.inference import parse_opt, check_img_size, \\\n",
    "    non_max_suppression, scale_boxes, increment_path, YOLOv9t_Seq, select_device, colors, yaml_load, letterbox, Annotator\n",
    "\n",
    "### \n",
    "# opt = parse_opt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65d61331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.modules import *\n",
    "def dump_layer(model:torch.Tensor, save_path:str):\n",
    "    with open(save_path, 'w') as f:\n",
    "        # shape C,H,W,N = 16,320,320,1\n",
    "        # type = f32 or f16\n",
    "        # model = model.permute(0, 1, 2, 3)\n",
    "\n",
    "        f.write(f\"# {model.shape[0]}, {model.shape[1]}, {model.shape[2]}, {model.shape[3]}\\n\")\n",
    "        f.write(f\"# {model.dtype}\\n\")\n",
    "        lines = model.detach().cpu()\n",
    "        lines = lines.permute(0, 2, 3, 1).numpy().reshape(-1)\n",
    "        lines = ' '.join([f\"{x:.4f}\" for x in lines])\n",
    "        f.write(f\"{lines}\")\n",
    "\n",
    "class YOLOv9t_Seq(nn.Module):\n",
    "    \"\"\"YOLOv9-t model implementation\"\"\"\n",
    "    def __init__(self, ch=3, nc=80):\n",
    "        super().__init__()\n",
    "        self.nc = nc\n",
    "        \n",
    "        # Sequential model with all layers\n",
    "        self.model = nn.Sequential(\n",
    "            Conv(ch, 16, 3, 2),              # 0-P1/2\n",
    "            Conv(16, 32, 3, 2),              # 1-P2/4\n",
    "            ELAN1(32, 32, 32, 16),           # 2\n",
    "            AConv(32, 64),                   # 3-P3/8\n",
    "            RepNCSPELAN4(64, 64, 64, 32, 3), # 4\n",
    "            AConv(64, 96),                   # 5-P4/16\n",
    "            RepNCSPELAN4(96, 96, 96, 48, 3), # 6\n",
    "            AConv(96, 128),                  # 7-P5/32\n",
    "            RepNCSPELAN4(128, 128, 128, 64, 3), # 8\n",
    "            SPPELAN(128, 128, 64),           # 9\n",
    "            \n",
    "            nn.Upsample(None, 2, 'nearest'), # 10\n",
    "            Concat(1),                       # 11-cat backbone P4(5)\n",
    "            RepNCSPELAN4(224, 96, 96, 48, 3), # 12\n",
    "            nn.Upsample(None, 2, 'nearest'), # 13\n",
    "            Concat(1),                       # 14 cat backbone P3(3)\n",
    "            RepNCSPELAN4(160, 64, 64, 32, 3), # 15 - N3 output\n",
    "            AConv(64, 48),                   # 16\n",
    "            Concat(1),                       # 17 cat head P4()\n",
    "            RepNCSPELAN4(144, 96, 96, 48, 3), # 18 - (P4/16-medium)\n",
    "            AConv(96, 64),                   # 19\n",
    "            Concat(1),                       # 20 cat head P5\n",
    "            RepNCSPELAN4(192, 128, 128, 64, 3), # 21 - (P5/32-large)\n",
    "        )\n",
    "        \n",
    "        # Detection head\n",
    "        self.detect = Detect(nc, (64, 96, 128))  # 22\n",
    "        self.stride = 32\n",
    "        # Save indices for feature extraction\n",
    "        self.save_indices = [3, 6, 9, 15, 18, 21]  # P3, P4, P5, N3, N4, N5\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with intermediate feature extraction\"\"\"\n",
    "        features  = {}\n",
    "        # Pass through each layer and save specific outputs\n",
    "        for i in range(len(self.model)):\n",
    "            layer = self.model[i]\n",
    "            \n",
    "            # Handle Concat operations\n",
    "            if isinstance(layer, Concat):\n",
    "                if i == 11:  # Upsample(10) + P4(6)\n",
    "                    x = layer([x, features[6]])\n",
    "                elif i == 14:  # Upsample(13) + P3(3)  \n",
    "                    x = layer([x, features[4]])\n",
    "                elif i == 17:  # AConv(16) + P4(12) ← 수정!\n",
    "                    x = layer([x, features[12]])  # layer 6이 아니라 12!\n",
    "                elif i == 20:  # AConv(19) + P5(9) ← 여기는 맞음\n",
    "                    x = layer([x, features[9]])\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            layer_name = layer.__class__.__name__\n",
    "            \n",
    "            # Save features needed for concat or output\n",
    "            # if i in [4, 6, 9, 12, 15, 18, 21]:\n",
    "            features[i] = x\n",
    "        for i in features.keys():\n",
    "            dump_layer(features[i], f\"torch_yolov9t_detections_features_layer_{i}.txt\")\n",
    "        # Multi-scale detection outputs\n",
    "        outputs = [features[15], features[18], features[21]]\n",
    "        \n",
    "\n",
    "        # Detection head\n",
    "        if self.training:\n",
    "            return self.detect(outputs)\n",
    "        else:\n",
    "            inference_out, raw_outputs = self.detect(outputs)\n",
    "            return inference_out, raw_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000d0878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "source = './tests/input/cat-and-hat.jpg'\n",
    "weights = './scripts/yolov9t_converted.pth'\n",
    "data = './tests/input/coco.yaml'\n",
    "imgsz = 640\n",
    "conf_thres = 0.25\n",
    "iou_thres = 0.45\n",
    "max_det = 1000\n",
    "device = ''\n",
    "half = False\n",
    "device = select_device(0)\n",
    "# model = DetectMultiBackend(opt.weights, device=device, data=opt.data, fp16=opt.half)\n",
    "\n",
    "model = YOLOv9t_Seq().cuda()\n",
    "model.load_state_dict(torch.load(weights, map_location='cpu', weights_only=False))\n",
    "imgsz = check_img_size(imgsz, s=model.stride)\n",
    "# Read image\n",
    "im0 = cv2.imread(source)  # BGR\n",
    "assert im0 is not None, f'Failed to load image: {source}'\n",
    "\n",
    "# Preprocess\n",
    "im = letterbox(im0, imgsz, stride=model.stride, auto=True)[0]\n",
    "model.eval()\n",
    "\n",
    "im = im.transpose((2, 0, 1))[::-1]  # HWC->CHW, BGR->RGB\n",
    "im = np.ascontiguousarray(im)\n",
    "names = yaml_load(data)['names']\n",
    "im_tensor = torch.from_numpy(im).to(device)\n",
    "im_tensor = im_tensor.half() if half else im_tensor.float()\n",
    "im_tensor /= 255.0\n",
    "if im_tensor.ndimension() == 3:\n",
    "    im_tensor = im_tensor.unsqueeze(0)\n",
    "dump_layer(im_tensor, f\"torch_yolov9t_detections_input_tensor.txt\")\n",
    "print(f'Image shape: {im_tensor.shape}')  # torch.Size([1, 3, 640, 640])\n",
    "# Inference\n",
    "# model.warmup(imgsz=(1, 3, *imgsz))\n",
    "pred = model(im_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d956d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 3, 640, 640])\n",
      "77.0 cat 0.96\n",
      "167.0 cat 0.96\n",
      "457.0 cat 0.96\n",
      "469.0 cat 0.96\n",
      "408.0 tv 0.50\n",
      "1.0 tv 0.50\n",
      "512.0 tv 0.50\n",
      "160.0 tv 0.50\n",
      "1.0 bed 0.25\n",
      "311.0 bed 0.25\n",
      "491.0 bed 0.25\n",
      "510.0 bed 0.25\n"
     ]
    }
   ],
   "source": [
    "print(f'Image shape: {im_tensor.shape}')  # torch.Size([1, 3, 640, 640])\n",
    "# Inference\n",
    "# model.warmup(imgsz=(1, 3, *imgsz))\n",
    "pred = model(im_tensor)\n",
    "# if not isinstance(pred, list):\n",
    "#     pred = [pred]\n",
    "def print_shape(p):\n",
    "    if isinstance(p, (list, tuple)):\n",
    "        print(\"is list\")\n",
    "        print(len(p))\n",
    "        for p in p:\n",
    "            print_shape(p)\n",
    "    else:\n",
    "        print(\"is not list\")\n",
    "        print(p.shape)\n",
    "\n",
    "# NMS\n",
    "pred = non_max_suppression(pred, conf_thres, iou_thres, max_det=max_det)\n",
    "# Draw boxes\n",
    "annotator = Annotator(im0.copy(), line_width=3, example=str(list(names.values())))\n",
    "\n",
    "det = pred[0]\n",
    "if len(det):\n",
    "    det[:, :4] = scale_boxes(im_tensor.shape[2:], det[:, :4], im0.shape).round()\n",
    "    for *xyxy, conf, cls in det:\n",
    "        c = int(cls)\n",
    "        cname = names[c] if c in names else str(c)\n",
    "        label = None if False else (cname if False else f'{cname} {conf:.2f}')\n",
    "        for i in xyxy:\n",
    "            print(i.detach().cpu().numpy(), label)\n",
    "        annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "\n",
    "result = annotator.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5100e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.imshow(result[:,:,::-1])\n",
    "plt.axis('off')  # 축 표시 제거 (선택사항)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716b06aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_layer = np.loadtxt('./torch_yolov9t_detections_input_tensor.txt', dtype=np.float32, delimiter=' ', skiprows=2)\n",
    "torch_layer = torch_layer.reshape(3, 640, 640, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b9b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gg_layer = np.loadtxt('./yolov9t_detections_input.txt', dtype=np.float32, delimiter=' ', skiprows=2)\n",
    "print(gg_layer.shape)\n",
    "gg_layer = gg_layer.reshape(3, 640, 640, 1)\n",
    "print(gg_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54746746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abs_diff = np.abs(torch_layer-gg_layer)\n",
    "diff_idx = np.argmax(abs_diff)\n",
    "diff_max = np.max(abs_diff)\n",
    "print(f\"Max diff: {diff_max} at index {diff_idx}\")\n",
    "\n",
    "gg_layer.flatten()[diff_idx], torch_layer.flatten()[diff_idx]\n",
    "\n",
    "torch_layer.mean(), gg_layer.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96213d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_txt_name = [f'torch_yolov9t_detections_features_layer_{i}.txt' for i in range(22)]\n",
    "cpp_txt_name = [f'yolov9t_detections_features_layer_{i}.txt' for i in range(22)]\n",
    "for t, c in zip(torch_txt_name, cpp_txt_name):\n",
    "    \n",
    "    # 첫번째 줄 '# 1, 16, 320, 320' 파싱하여 reshape\n",
    "    torch_layer = np.loadtxt(t, dtype=np.float32, delimiter=' ', skiprows=2)\n",
    "    def get_shape_from_file(t):\n",
    "        with open(t, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "        shape_str = first_line.lstrip('# ').strip()\n",
    "        shape = tuple(map(int, shape_str.split(',')))\n",
    "        return shape\n",
    "    shape = get_shape_from_file(t)\n",
    "    # shape = tuple(reversed(shape))\n",
    "    N, C, H, W = shape\n",
    "    print(shape)\n",
    "    # numpy.ascontiguousarray — NumPy v2.4.dev0 Manual16, 320, 320, 1\n",
    "    torch_layer = np.ascontiguousarray(torch_layer.reshape(C, H, W, N))\n",
    "    # torch_layer = torch_layer.transpose(0, 3, 2, 1)  # NCHW -> NHWC\n",
    "    ###############################\n",
    "    # ggml_type = NUMPY_DTYPE_TO_GGML_TYPE[x.dtype.type]\n",
    "    #     shape = tuple(reversed(x.shape))\n",
    "    #     tensor = ggml.ggml_new_tensor(\n",
    "    #         ctx,\n",
    "    #         ggml_type.value,\n",
    "    #         len(shape),\n",
    "    #         (ctypes.c_int64 * len(shape))(*shape),\n",
    "    #     )\n",
    "    #     tensor.contents.nb[: len(shape)] = (ctypes.c_int64 * len(shape))(\n",
    "    #         *tuple(reversed(x.strides))\n",
    "    #     )\n",
    "    #     if ggml.ggml_get_data(tensor) is not None\n",
    "    \n",
    "    # 번째 줄 '# 16, 320, 320, 1' 파싱하여 reshape\n",
    "    cpp_layer = np.loadtxt(c, dtype=np.float32, delimiter=' ', skiprows=2)\n",
    "    shape = get_shape_from_file(c)\n",
    "    C, W, H, N = shape\n",
    "    \n",
    "    print(shape)\n",
    "    cpp_layer = cpp_layer.reshape(shape)\n",
    "    # cpp_layer = np.transpose(cpp_layer, (3, 2, 1, 0))\n",
    "    # cpp_layer = cpp_layer.transpose(3, 2, 1, 0)  # CHWN -> NHWC\n",
    "    if torch_layer.shape != cpp_layer.shape:\n",
    "        print(f\"Shape mismatch: {torch_layer.shape} vs {cpp_layer.shape}\")\n",
    "        break\n",
    "    abs_diff = np.abs(torch_layer-cpp_layer)\n",
    "    diff_idx = np.argmax(abs_diff)\n",
    "    diff_max = np.max(abs_diff)\n",
    "    print(f\"Max diff: {diff_max} at index {diff_idx}\")\n",
    "    # diff mean, std\n",
    "    print(f\"Mean diff: {np.mean(abs_diff)}, Std diff: {np.std(abs_diff)}\")\n",
    "    # if diff_max > 1e-5:\n",
    "    print(cpp_layer.shape)\n",
    "    print(torch_layer.shape)\n",
    "    break\n",
    "    #     print(f\"  Torch value: {torch_layer[diff_idx]}, C++ value: {cpp_layer[diff_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db27f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 16, 320, 320, 1 의 1번째 차원과 2번째 차원의 축의 내용을 바꿔줘.\n",
    "# torch_layer = np.swapaxes(torch_layer, 1, 2)\n",
    "# torch_layer = np.ascontiguousarray(torch_layer)\n",
    "# torch_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gguf.gguf_reader import GGUFReader\n",
    "reader = GGUFReader('./models/yolov9t_converted.gguf')\n",
    "for k, v in reader.fields.items():\n",
    "    print(f\"{k}\")\n",
    "for tensors in reader.tensors:\n",
    "    name = tensors.name\n",
    "    shape = tensors.shape\n",
    "    data = tensors.data\n",
    "    print(name, shape, data.shape)\n",
    "    break\n",
    "a = torch.load('./scripts/yolov9t_converted.pth', map_location='cpu', weights_only=False)\n",
    "print(a[name].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde96a19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-cpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
